{"version":3,"file":"Tokenizer.js","sourceRoot":"","sources":["Tokenizer.ts"],"names":[],"mappings":"AAAA;;;;GAIG;AAEH,YAAY,CAAC;;;;AAEb,qDAA2C;AAE3C,+BAAyD;AACzD,sEAAsC;AAc/B,IAAe,mBAAmB,GAAlC,MAAe,mBAAoB,SAAQ,gBAAU;IAArD;;QAGmB,SAAI,GAAG,WAAW,CAAC;IAwF7C,CAAC;IApFgB,IAAI,CAAC,OAAgB,EAAE,GAAG,IAAI;QAE7C,KAAK,CAAC,IAAI,CAAC,OAAO,EAAE,GAAG,IAAI,CAAC,CAAC;QAE7B,OAAO,IAAI,CAAC;IACb,CAAC;IAEM,MAAM,CAAU,IAAI,CAAsD,OAAgB,EAAE,GAAG,IAAI;QAEzG,aAAa;QACb,OAAO,KAAK,CAAC,IAAI,CAAI,OAAO,EAAE,GAAG,IAAI,CAAC,CAAC;IACxC,CAAC;IAED;;;OAGG;IACO,WAAW,CAAuC,KAAU,EAAE,EAAkC;QAEzG,qCAAqC;QAErC,EAAE,GAAG,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAEnB,IAAI,GAAG,GAAG,EAAE,CAAC;QACb,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,IAAI,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAC1C,CAAC;YACA,IAAI,OAAO,IAAI,CAAC,CAAC,KAAK,QAAQ,EAC9B,CAAC;gBACA,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAChB,CAAC;iBAED,CAAC;gBACA,IAAI,SAAS,GAAG,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;gBAE3B,IAAI,IAAA,iBAAO,EAAC,SAAS,CAAC,EACtB,CAAC;oBACA,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBAChB,CAAC;qBAED,CAAC;oBACA,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;gBAC7B,CAAC;YACF,CAAC;QACF,CAAC;QAED,OAAO,GAAG,CAAC;IACZ,CAAC;IAED;;;OAGG;IACO,YAAY,CAAuC,KAAU,EAAE,EAAkC;QAE1G,qCAAqC;QAErC,EAAE,GAAG,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAEnB,IAAI,GAAG,GAAG,EAAE,CAAC;QACb,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,IAAI,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAC1C,CAAC;YACA,IAAI,IAAI,CAAC,CAAC,EACV,CAAC;gBACA,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAChB,CAAC;iBAED,CAAC;gBACA,wCAAwC;gBACxC,IAAI,SAAS,GAAG,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;gBAE3B,IAAI,IAAA,iBAAO,EAAC,SAAS,CAAC,EACtB,CAAC;oBACA,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBAChB,CAAC;qBAED,CAAC;oBACA,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;gBAC7B,CAAC;YAEF,CAAC;QACF,CAAC;QAED,OAAO,GAAG,CAAC;IACZ,CAAC;;AA1FoB,kDAAmB;AAER,wBAAI,GAAG,WAAW,AAAd,CAAe;8BAF9B,mBAAmB;IAFxC,0BAAQ;IACT,aAAa;GACS,mBAAmB,CA2FxC;AAED;;GAEG;AACH,MAAa,SAAU,SAAQ,aAAO;IAAtC;;QAEU,SAAI,GAAG,WAAW,CAAC;IAsC7B,CAAC;IApCA;;;;;;OAMG;IACH,KAAK,CAAC,IAAY,EAAE,IAAqB,EAAE,GAAG,IAAI;QAEjD,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,EACnB,CAAC;YACA,MAAM,KAAK,CAAC,sBAAsB,CAAC,CAAC;QACrC,CAAC;aAED,CAAC;YACA,IAAI,GAAG,GAAY,CAAC,EAAE,CAAC,EAAE,IAAI,EAAE,CAAC,CAAC;YAEjC,OAAO,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,GAAG,EAAE,IAAI,EAAE,GAAG,IAAI,CAAC,CAAC;YAEnD;;;;;;;;;;;;;;cAcE;QACH,CAAC;IACF,CAAC;CACD;AAxCD,8BAwCC;AAED,kBAAe,SAAS,CAAC","sourcesContent":["/**\n * 分词模块管理器\n *\n * @author 老雷<leizongmin@gmail.com>\n */\n\n'use strict';\n\nimport { autobind } from 'core-decorators';\nimport { IWord, Segment } from '../Segment';\nimport { ISubSModule, SModule, SubSModule } from './mod';\nimport isUnset from '../util/isUnset';\n\nexport type ISubTokenizer = ISubSModule & {\n\ttype: 'tokenizer',\n\tsplit(words: IWord[], ...argv): IWord[],\n}\n\nexport type ISubTokenizerCreate<T extends SubSModuleTokenizer, R extends SubSModuleTokenizer = SubSModuleTokenizer> = {\n\t(...argv: Parameters<T[\"init\"]>): T & R,\n\t(segment: Segment, ...argv): T & R,\n};\n\n@autobind\n// @ts-ignore\nexport abstract class SubSModuleTokenizer extends SubSModule implements ISubTokenizer\n{\n\tpublic static override readonly type = 'tokenizer';\n\tpublic override readonly type = 'tokenizer';\n\n\tpublic abstract split(words: IWord[], ...argv): IWord[]\n\n\tpublic override init(segment: Segment, ...argv)\n\t{\n\t\tsuper.init(segment, ...argv);\n\n\t\treturn this;\n\t}\n\n\tpublic static override init<T extends SubSModuleTokenizer = SubSModuleTokenizer>(segment: Segment, ...argv): T\n\t{\n\t\t// @ts-ignore\n\t\treturn super.init<T>(segment, ...argv);\n\t}\n\n\t/**\n\t * 仅对未识别的词进行匹配\n\t * 不包含 p 為 0\n\t */\n\tprotected _splitUnset<T extends IWord, U extends IWord = T>(words: T[], fn: (text: string, ...argv) => U[]): U[]\n\t{\n\t\t//const POSTAG = this.segment.POSTAG;\n\n\t\tfn = fn.bind(this);\n\n\t\tlet ret = [];\n\t\tfor (let i = 0, word; word = words[i]; i++)\n\t\t{\n\t\t\tif (typeof word.p === 'number')\n\t\t\t{\n\t\t\t\tret.push(word);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tlet words_new = fn(word.w);\n\n\t\t\t\tif (isUnset(words_new))\n\t\t\t\t{\n\t\t\t\t\tret.push(word);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tret = ret.concat(words_new);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn ret;\n\t}\n\n\t/**\n\t * 仅对未识别的词进行匹配\n\t * 包含已存在 但 p 為 0\n\t */\n\tprotected _splitUnknow<T extends IWord, U extends IWord = T>(words: T[], fn: (text: string, ...argv) => U[]): U[]\n\t{\n\t\t//const POSTAG = this.segment.POSTAG;\n\n\t\tfn = fn.bind(this);\n\n\t\tlet ret = [];\n\t\tfor (let i = 0, word; word = words[i]; i++)\n\t\t{\n\t\t\tif (word.p)\n\t\t\t{\n\t\t\t\tret.push(word);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t//let words_new = fn.call(this, word.w);\n\t\t\t\tlet words_new = fn(word.w);\n\n\t\t\t\tif (isUnset(words_new))\n\t\t\t\t{\n\t\t\t\t\tret.push(word);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tret = ret.concat(words_new);\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\n\t\treturn ret;\n\t}\n}\n\n/**\n * 分词模块管理器\n */\nexport class Tokenizer extends SModule\n{\n\toverride type = 'tokenizer';\n\n\t/**\n\t * 对一段文本进行分词\n\t *\n\t * @param {string} text 文本\n\t * @param {array} modules 分词模块数组\n\t * @return {array}\n\t */\n\tsplit(text: string, mods: ISubTokenizer[], ...argv)\n\t{\n\t\tif (mods.length < 1)\n\t\t{\n\t\t\tthrow Error('No tokenizer module!');\n\t\t}\n\t\telse\n\t\t{\n\t\t\tlet ret: IWord[] = [{ w: text }];\n\n\t\t\treturn this._doMethod('split', ret, mods, ...argv);\n\n\t\t\t/*\n\t\t\t// 按顺序分别调用各个module来进行分词 ： 各个module仅对没有识别类型的单词进行分词\n\t\t\tmods.forEach(function (mod)\n\t\t\t{\n\t\t\t\t// @ts-ignore\n\t\t\t\tif (typeof mod._cache == 'function')\n\t\t\t\t{\n\t\t\t\t\t// @ts-ignore\n\t\t\t\t\tmod._cache();\n\t\t\t\t}\n\n\t\t\t\tret = mod.split(ret, ...argv);\n\t\t\t});\n\t\t\treturn ret;\n\t\t\t*/\n\t\t}\n\t}\n}\n\nexport default Tokenizer;\n"]}